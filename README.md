# üõ´ Pipeline de Dados com Apache Airflow

Este reposit√≥rio cont√©m o projeto desenvolvido durante o curso da Alura.

## üìå Descri√ß√£o do Projeto

Neste projeto, foi criado um pipeline de dados para uma empresa de turismo, com o objetivo de obter semanalmente dados da previs√£o do tempo. O fluxo √© executado **automaticamente toda segunda-feira**, orquestrado por uma DAG no Apache Airflow.

Os dados s√£o coletados por meio de uma API, tratados com Python e armazenados em pastas organizadas por semana. Este processo permite √† empresa planejar melhor seus servi√ßos com base nas condi√ß√µes clim√°ticas previstas.

## ‚öôÔ∏è Tecnologias e Ferramentas Utilizadas

- **Apache Airflow**: ferramenta de orquestra√ß√£o de workflows
- **Python**: para manipula√ß√£o dos dados
- **Pandas**: para leitura e tratamento de arquivos CSV
- **BashOperator & PythonOperator**: para cria√ß√£o das tasks
- **Visual Crossing API**: para acesso √†s informa√ß√µes clim√°ticas

## üöÄ O que foi aprendido

Durante o curso, foram abordados os seguintes t√≥picos:

- Conceitos fundamentais do Apache Airflow: DAGs, Tasks, Operators
- Estrutura e componentes da arquitetura do Airflow
- Instala√ß√£o e configura√ß√£o do Airflow em ambiente local
- Cria√ß√£o de DAGs agendadas com `schedule_interval`
- Uso do BashOperator para cria√ß√£o de diret√≥rios
- Uso do PythonOperator para execu√ß√£o de fun√ß√µes de extra√ß√£o e salvamento de dados
- Organiza√ß√£o e modulariza√ß√£o de um pipeline de dados automatizado



